Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	colours
	1	coordinates
	1	filter_metadata
	1	geoscheme
	1	preanalyses
	5

[Thu Apr 16 13:51:29 2020]
Job 2: 
		Processing pre-analyses/temp_sequences.fasta to:
		- Filter only lines corresponding to genomes included in pre-analyses/temp_sequences.fasta
		- Reformat metadata by dropping or adding columns, and fixing some fields
		

[Thu Apr 16 13:51:31 2020]
Finished job 2.
1 of 5 steps (20%) done

[Thu Apr 16 13:51:31 2020]
Job 1: 
		Processing pre-analyses/metadata_filtered.tsv to:
		- Reformat columns according to geographic scheme in config/geoscheme.xml
		

[Thu Apr 16 13:51:32 2020]
Finished job 1.
2 of 5 steps (40%) done

[Thu Apr 16 13:51:32 2020]
Job 3: 
		Searching for coordinates (latitudes and longitudes) for samples in data/metadata.tsv
		

[Thu Apr 16 13:52:17 2020]
Finished job 3.
3 of 5 steps (60%) done

[Thu Apr 16 13:52:17 2020]
Job 4: 
		Assigning colour scheme for samples in data/metadata.tsv based on geoscheme
		

[Thu Apr 16 13:52:19 2020]
Finished job 4.
4 of 5 steps (80%) done

[Thu Apr 16 13:52:19 2020]
localrule preanalyses:
    input: data/metadata.tsv, data/sequences.fasta, config/latlongs.tsv, config/colors.tsv
    jobid: 0

[Thu Apr 16 13:52:19 2020]
Finished job 0.
5 of 5 steps (100%) done
Complete log: /Users/anderson/GLab Dropbox/Anderson Brito/projects/ncov/nextstrain/20200416_update2/.snakemake/log/2020-04-16T135129.466583.snakemake.log
